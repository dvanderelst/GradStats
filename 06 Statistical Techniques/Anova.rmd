---
title: "The LM with categorical predictors (ANOVA)"
date: "Last Updated: `r format(Sys.time(), '%d, %B, %Y at %H:%M')`"
output:
  github_document:
    html_preview: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(reshape)
```

# One way Anova: one categorical predictor

## Fake data

### Make some population data

```{r}
sample_size <- 15

population1 <- rnorm(100000, mean = 7.5, sd = 2)
population2 <- rnorm(100000, mean = 10, sd = 2)
population3 <- rnorm(100000, mean = 6, sd = 2)

hist(population1, breaks=100, col=rgb(1,0,0,0.2), main='', xlab = 'Penguin Weight')
hist(population2, breaks=100,  add=TRUE, col=rgb(0,1,0,0.2))
hist(population3, breaks=100,  add=TRUE, col=rgb(0,0,1,0.2))
title('Populations 1, 2, and 3')
```

### Sample from the populations

```{r}


france <- sample(population1, sample_size)
senegal <- sample(population2, sample_size)
japan <- sample(population3, sample_size)

hist(france, breaks=100, col=rgb(1,0,0,0.2), main='', xlab='Penguin Weight')
hist(senegal, breaks=100,  add=TRUE, col=rgb(0,1,0,0.2))
hist(japan, breaks=100,  add=TRUE, col=rgb(0,0,1,0.2))
title('Samples 1, 2, and 3')

data <-data.frame(cbind(france,senegal,japan))
data<-melt(data, id.vars=c())
colnames(data) <- c('sample', 'weight')
data$sample<-factor(data$sample)
head(data)
tail(data)
```

```{r}
boxplot(weight ~ sample, data = data)
```

### Run classic ANOVA

```{r}
result <- aov(weight ~ sample, data = data)
summary(result)
```

### Run LM

```{r}
result <- lm(weight ~ sample, data = data)
summary(result)
```

### Change contrasts

#### Helmert coding

Here, I use Helmert coding (from R). Helmert coding compares each level of a categorical variable to the mean of the subsequent levels.

Compare this to the slides.

```{r}
contrasts(data$sample) <- contr.helmert(n = 3)
result <- lm(weight ~ sample, data = data)
summary(result)
```

```{r}
data
model.matrix(result)
```

#### Compare coefficients with data

```{r}
# Fitted coefficients
result$coefficients
```

```{r}


all_data <- c(france, senegal, japan)
grand_mean <- mean(all_data)
mean_france <- mean(france)
mean_senegal <- mean(senegal)
mean_japan <- mean(japan)

c(grand_mean, mean_france, mean_senegal, mean_japan)
```

In the Helmert coding (The R version)...

-   The intercept is the grand mean
-   Beta1 is 0.5 X the difference between France and Senegal
-   Beta2 is 1/3 X the difference between Japan and the mean of Senegal and France

```{r}
intercept <- grand_mean
beta1 <- (mean_senegal - mean_france) / 2
beta2 <- (mean_japan - (mean_senegal + mean_france) / 2) / 3
c(intercept, beta1, beta2)

result$coefficients
```

#### Using the Faux package to change contrasts

```{r}
library(faux)
helmert <- contr_code_helmert(c(1,2,3))
helmert <- contrasts(helmert)

contrasts(data$sample)  <- helmert

result <- lm(weight ~ sample, data = data)
summary(result)
c(grand_mean, mean_france, mean_senegal, mean_japan)
```

## Real data: feet

```{r}
library(tidyverse)
feet <- read_csv('data/feet.csv')
head(feet)
```

```{r}
model <- lm(FootLength ~ Sex, data = feet)
summary(model)
```

```{r}
boxplot(FootLength ~ Sex, data = feet)
```

## Real data: flies (A warning!)

<http://jse.amstat.org/datasets/fruitfly.txt>

```{r}
flies <- read_csv('data/flies.csv')
head(flies)

# Don't do this!!
model <- lm(LONGEVITY ~ TYPE, data = flies)
summary(model)
## Do this!!
model <- lm(LONGEVITY ~ as.factor(TYPE), data = flies)
summary(model)
```

```{r}
boxplot(LONGEVITY ~ TYPE, data = flies)
```

# Two way ANOVA: two categorical predictors

## Real data: car noise

```{r}
library(Stat2Data)
data(AutoPollution)
head(AutoPollution)
AutoPollution$Size <- as.factor(AutoPollution$Size) 
AutoPollution$Type <- as.factor(AutoPollution$Type) 
levels(AutoPollution$Size)
levels(AutoPollution$Type)
```

```{r}
interaction.plot(x.factor=AutoPollution$Type, trace.factor = AutoPollution$Size, response=AutoPollution$Noise)
```

### Model without interaction

When we fit a model without interactions, its equation is the following:

$$
y = \beta_0 + [\beta_1 x_1 + \beta_2 x_2 ] + [\beta_3 x_3]
$$

with $x_1$ and $x_2$ encoding the Variable `Size` and $x_3$ encoding the variable `Type`.

```{r}
model <- lm(Noise ~ Type + Size, data = AutoPollution)
summary(model)
```

### Model with interaction

When we fit a model **with** interactions, its equation is the following:

$$
y = \beta_0 + [\beta_1 x_1 + \beta_2 x_2 ] + [\beta_3 x_3] + [\beta_4 x_1 x_3] + [\beta_5 x_2 x_3]
$$

with $x_1$ and $x_2$ encoding the Variable `Size` and $x_3$ encoding the variable `Type`, and two terms that encode the combination of the levels of `Size` and `Type`.

```{r}
model <- lm(Noise ~ Type * Size, data = AutoPollution)
summary(model)
```

## Real data: fat rats

```{r}
data(FatRats)
head(FatRats)
```

+ `Protein`: Level of protein (Hi or Lo)
+ `Source`: Source of protein (Beef, Cereal, or Pork)

```{r}
model<-lm(Gain ~ Protein * Source, data=FatRats)
summary(model)
```

```{r}
interaction.plot(x.factor=FatRats$Source, trace.factor = FatRats$Protein, response= FatRats$Gain)
```

### Change the contrasts

Here, I just quickly change the contrasts. This does not change the model's equation (and number of fitted coefficients) but it does change the meaning of these coefficients. The sum coding compares levels to the overall mean of the response variable (the intercept is the overall mean), i.e., 

```{r}
mean(FatRats$Gain)
```


```{r}
contrasts(FatRats$Protein) <- contr.sum(2)
contrasts(FatRats$Source) <- contr.sum(3)
model<-lm(Gain ~ Protein + Source, data=FatRats)
summary(model)
```

### Interpreting the coefficients

The sum contrast compares each level of a variable to the overall mean. It is important to remember that knowing a variable's mean value and n deviations from this mean allows us to fix n + 1 values. This mechanism is used by deviation coding to model the various levels of the dependent given the independent.

The overall mean for the dependent is 87.9. The fitted coefficient for `Protein` is 7.233. This means that for one level of this variable the average should be (87.9 + 7.233) = 95.133. For the other level, the average should be 87.9 - 7.233) = 80.667. Indeed, if this were not so, the overall mean could not be 87.9. 

We can see that is so by calculating the average of the dependent variable for the two levels of `Protein`:

```{r}
grouped <- group_by(FatRats, Protein)
means <- summarise(grouped, mean = mean(Gain))
means
```

For the variable `Source` a similar reasoning holds.

For one level of the variable, the average dependent should be (87.9 +1.7) = 89.6. For another, the average should be (87.9 -3) = 84.9. Finally, to ensure that the average is 87.9, the last level should have a mean of 87.9 - (1.7 + -3) = 89.2.

```{r}
grouped <- group_by(FatRats, Source)
means <- summarise(grouped, mean = mean(Gain))
means
```