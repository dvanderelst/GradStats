---
title: "Filtering Data"
date: "Last Updated: `r format(Sys.time(), '%d, %B, %Y at %H:%M')`"
output:
  github_document:
    html_preview: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Before we begin...

Download the following data files to your computer:

-   `transit-data.xlsx`
-   `Titanic.csv`
-   `cars.txt`
-   `films.dat`

## dplyr: Selecting rows and columns

dplyr uses two functions to select columns and filter rows: `select()` and `filter()`.

```{r}
library(tidyverse)
library(readxl)
```

Let's read in some data and make sure that the variable name adhere to R's expectations.

```{r}
data <- read_excel("data/transit-data.xlsx", sheet = 'transport data', skip=1)
colnames(data) <- make.names(colnames(data))
```

### Selecting columns

Selecting columns is usally less usefull than filtering rows (which I will cover below). However, it can be used to create a subset of the data that is easier to work with. I tend to use this to split off specific parts of the data that need cleaning or transformation before merging them again. For example, I might clean demographic data for a survey separately from the response data.

#### Single variable

Selecting columns is very straightforward.

```{r}
subset <- select(data, date, sender.latitude)
head(subset)
```

#### Multiple variables

Selecting multiple columns

```{r}
subset <- select(data, date, c(sender.latitude, sender.longitude))
head(subset)
```

#### Unselecting variables

You can also unselect columns

```{r}
colnames(data)
subset <- select(data, -number.of.items)
colnames(subset)

```

#### Other functions to select variable names

`starts_with()` selects columns that start with a specific prefix.

```{r}
titanic_data <- read_csv("data/Titanic.csv")
subset <- select(titanic_data, starts_with("S"))
colnames(subset)
```

`ends_with()` selects columns that end with a specific suffix.

```{r}
titanic_data <- read_csv("data/Titanic.csv")
subset <- select(titanic_data, ends_with("e"))
colnames(subset)
```

`contains()` selects columns that contain a specific string.

```{r}
titanic_data <- read_csv("data/Titanic.csv")
subset <- select(titanic_data, contains("e"))
colnames(subset)
```

`matches()` selects columns that match a regular expression.

(This is often useful when the data contain columns with similar names that follow a pattern.)

```{r}
titanic_data <- read_csv("data/Titanic.csv")
subset <- select(titanic_data, matches("^[NA]"))
colnames(subset)
```

Explanation of the regular expression:

+ `^`: This is the anchor that signifies the start of the string. It ensures that the match happens at the beginning of the column name, not somewhere in the middle or end.
+ `[NA]`: This defines a character class. It matches either the letter `N` or `A`. The square brackets `[]` are used to specify multiple characters, meaning the regular expression will match either of the characters listed inside the brackets.

So, `^[NA]` will match any string (or column name, in this case) that starts with either N or A.

There are even more way of using the select function together with other functions, including `num_range()`, `one_of()`, `everything()`, and `last_col()`. See the R documentation for more details: <https://dplyr.tidyverse.org/reference/select.html>

### Filtering rows

```{r}
subset <- filter(data, sender.latitude < 50)
head(subset)
```

You can run filters in succession for more complex filtering

```{r}
subset <- filter(data, sender.latitude < 50)
subset <- filter(subset, sender.latitude > 32)
subset <- filter(subset, sender.longitude > 0)
subset <- select(subset,  sender.latitude, sender.longitude)
summary(subset)
```

You can string queries together using `&`, `|`, and others. See `Logical Operators` in the R documentation for details.

```{r}
hist(data$sender.longitude, breaks=25)
subset <- filter(data, sender.longitude < -50 | sender.longitude > 100)
hist(subset$sender.longitude,breaks=25)
```

Let's use the NOT operator to filter out one of the locations.

```{r}
unique(data$sender.location)
subset <- filter(data, sender.location != 'USA, Cincinnati (OH)')
unique(subset$sender.location)
```

One handy trick is to select using a list of values.

```{r}
selected_locations <-c('USA, Brownfield (TX)', 'USA, Rochester (NY)', 'USA, Wellington (KS)')
subset <- filter(data, sender.location %in% selected_locations)
unique(subset$sender.location)
```
### Special cases

#### Filtering missing values

```{r}
library(skimr)

# Let's load in a dataset with missing values
pakistan_data <- read_csv("data/pakistan_intellectual_capital.csv")
colnames(pakistan_data) <- make.names(colnames(pakistan_data))
skim(pakistan_data)

pakistan_data <- filter(pakistan_data, !is.na(Year))
skim(pakistan_data)
```

#### Remove duplicates based on columns

```{r}
pakistan_data <- read_csv("data/pakistan_intellectual_capital.csv")
colnames(pakistan_data) <- make.names(colnames(pakistan_data))
skim(pakistan_data)
# Let's check whether the data contains duplicates
# We will assume that people are unique if they have a different terminal degree and graduated from different institutions, and have different names.
pakistan_data <- distinct(pakistan_data, Teacher.Name, Terminal.Degree, Graduated.from)
skim(pakistan_data)
```

### Ordering rows

You can use `arrange` to order the rows.

```{r}
subset1 <-select(data, sender.location, sender.latitude)
subset1 <-arrange(subset1, sender.latitude)
head(subset1, n = 15)
```

Arranging in descending order? No problem.

```{r}
subset1 <-select(data, sender.location, sender.latitude)
subset1 <-arrange(subset1, desc(sender.latitude))
head(subset1, n = 15)
```

Arranging based on multiple columns.

```{r}
subset1 <-select(data, sender.location, sender.latitude)
subset1 <-mutate(subset1, rounded = round(sender.latitude))
subset1 <-arrange(subset1, desc(rounded), sender.location)
head(subset1, n = 15)
```

## Exercises

### Exercise: Titanic Data

Read in the `Titanic.csv` data and perform the following operations:

-   Count the number of male survivors that were older than 25.
-   How many first class passengers were female?
-   How many female passengers survived?

### Exercise: Car data

Read in the `cars.txt` data and perform the following operations:

The data file contains variables describing a number of cars.

-   Select all cars with at least 25 mpg in the city.
-   Select all BMW's
-   Are there any Large cars with more than 25 mpg in the city?
-   Which cars use over 50% more fuel on the city than they do in the highway?
-   Which cars have an action radius of over 400 miles on the highway?

### Exercise: Film data

Use the following data file: `films.dat` (in the data folder). This file lists the title, year of release, length in minutes, number of cast members listed, rating, and number of lines of description are recorded for a simple random sample of 100 movies.

-   Write code to select all films from 1980 to 1990 (including both 1980 and 1990)

-   Select all films with a rating of 1

-   Write a short script that allows selecting all movies that were made in the five years before a given date. The script starts by assigning a value (year) to a variable. The script selects all movies made in the 5 years preceding the year assigned to the variable and prints the selected data to the screen. The earliest film in the data was made in 1924. Therefore, if the year assigned to the variable is before 1930, the script should print the message `No movies found`.

-   Write code that adds a new variable `ratio` to the data. This variable is obtained by dividing the number of actors (`Cast`) by the length of the movie (`Length`). Next, select the movies for which the ratio Cast/Length is at least 0.1. Print the selected movies to the screen.
