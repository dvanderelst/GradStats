---
title: "Cleaning Data"
date: "Last Updated: `r format(Sys.time(), '%d, %B, %Y at %H:%M')`"
output:
  github_document:
    html_preview: true
    toc: true
---

## Before we start...

Download the following data sets:

-   `raw_depression.csv`
-   `cars.txt`
-   `DEMO_J.XPT`
-   `INQ_J.XPT`
-   `DUQ_J.XPT`
-   `ssadisability.csv`
-   `coal.csv`
-   `vot.csv`
-   `airline-safety.csv`
-   `Titanic.csv`
-   `adult.csv`

## Loading the tidyverse

Let's load the tidyverse...

```{r}
library(tidyverse)
library(skimr)
```

## Some Simple Operations

### Opening Large Text Files

One of the most simple things you can do is inspect your data using a text editor. When text files are large, it's a good idea to get an editor that can handle large files. I suggest [Sublime Text Editor](https://www.sublimetext.com/). This editor can handle much larger files than you can open (as text) in Rstudio. Keep the file open while you're cleaning it!

### Detecting Errors in Data

Errors can be most easily identified using graphs (See later). However, some textual output might be useful as well.

Let's read in some data (see `codebook.txt` for info about the variables).

```{r}
depression_data <- read_tsv('data/raw_depression.csv')
```

People have clearly made errors when entering their ages:

```{r}
skim(depression_data$age)
```

The histogram looks weird because of the very few, very high ages. We *could* try to fix these by assuming, for example, that `1998` is the birth year and not the age. For now, let's just remove these entries.

```{r}
hist(depression_data$age)
```

```{r}
dim(depression_data) # Get the dimensions of the data before filtering
filtered_depression_data <- filter(depression_data, age < 120)
dim(filtered_depression_data) # Get the dimensions of the data after filtering
hist(filtered_depression_data$age, breaks=100)
```

Here is another visualization.

```{r}
par(mfcol=c(1,2))
boxplot(depression_data$age)
boxplot(filtered_depression_data$age)
```

Here is another problem with the data. We could try to fix this by assuming that people have made mistakes when entering their family size. Just as before, we could remove these entries.

```{r}
hist(filtered_depression_data$familysize, breaks=100)
summary(filtered_depression_data$familysize)
```

```{r}
filtered_depression_data <- filter(filtered_depression_data, familysize < 20)
hist(filtered_depression_data$familysize)
summary(filtered_depression_data$familysize)
```


Categorical data can also be messy. People entered their majors as free text. And this is the result:

```{r}
library(janitor)
result <- tabyl(depression_data$major)
head(result, 50)
```
Cleaning this would be more laborious and will require text processing. We will cover some of this later.

### Missing Data

R encodes missing values as `NA`. Missing values are contagious: performing calculations on data that contains missing values often leads to an `NA` result. Some functions are robust against missing values (they simply ignore them). However, when a result comes back as `NA` it's often because the data contained missing values in the first place.

```{r}
some_data <- c(1,2,3,4, NA)
mean(some_data)
max(some_data)
```

In the section on Filtering Data, we have covered how to filter out missing values.

Tips from the field:

-   When reading in a file, it's a good idea to know how missing values are encoded so you can tell R which values to interpret as missing. We have covered this under "Reading Data".
-   Sometime people will use absurd values to indicate missing data. For example, they might enter -1 or 99999 when an age value is missing. These are then read by R as numbers and your calculations end up being nonsense. One way to catch this it by looking for outliers!

Here is an example of the last case.

```{r}
income_data <- read_csv('data/adult.csv')
colnames(income_data) <- make.names(colnames(income_data))
```

It seems the value `9999` is used to indicate missing data for the variable `capital gain`.

```{r}
# Create a table for the various values for the variable 'capital-gain'
tbl<-tabyl(income_data$capital.gain)
# Show the last ten rows of tbl
tail(tbl)
```

Likewise, the value `?` seems to be used to indicate missing data for the variable `occupation`.

```{r}
# Create a table for the various values for the variable 'occupation'
tbl <- tabyl(income_data$occupation)
head(tbl)
```


### Separating a column

It happens more often that you would exspect that variables are combined in one column. For example, the column `Hospital.Referral.Region.Description` in the `inpatient` data set combines the state and city of the hospital. We can separate this column into two columns.

```{r}
patient_data <-read_tsv('data/inpatient.tsv')
colnames(patient_data) <- make.names(colnames(patient_data))
head(patient_data, 5)
```

```{r}
result <- tabyl(patient_data$Hospital.Referral.Region.Description)
head(result, 5)
```

Let's separate this column into two columns.

```{r}
test <- separate(patient_data, Hospital.Referral.Region.Description, into=c('state', 'city'), sep='-')
```

This resulted in some warnings in certain rows. Let's look at some of those rows.

```{r}
patient_data$Hospital.Referral.Region.Description[710]
patient_data$Hospital.Referral.Region.Description[711]
patient_data$Hospital.Referral.Region.Description[1930]
```

The problem seems to be that these rows contain values with more than one `-`. Let's look at an example of the results.


```{r}
test$state[710]
test$city[710]
```

It turns out that R has remove the second part of the city name. We can fix this setting the `remove` argument to `FALSE`.

```{r}
test <- separate(patient_data, Hospital.Referral.Region.Description, into=c('state', 'city'), sep=' - ', remove = FALSE)
```

No warnings anymore! Let's look at an example of the results again.

```{r}
test$state[710]
test$city[710]
```

Note, you can use regular expression for the `sep` argument: [see here.](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html). This allows you to separate columns based on more complex patterns.

### Combining columns

The opposite operation!

The remove argument in the `unite()` function specifies whether the original columns should be removed after creating the new combined column. So, the `remove` argument does something different than in the `separate()` function.

+ `remove = TRUE` (default): The original columns are removed from the data frame after combining them.
+ `remove = FALSE`: The original columns are retained alongside the new combined column.

```{r}
test <- unite(test, 'combined', c(Provider.Id, Provider.Name), sep='_+_', remove = FALSE)
head(test$combined, 3)
```

## Strings using `stringr`

It does happen that you need to clean textual data. The `stringr` package has a bunch of functions to make your life easier (but not easy). I will run through some examples but do have a look at the [cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) as well.

Remember this data? We will try to clean the `major` column to a certain extent. We will convert the majors to lower case and try to find biology majors. Fully cleaning this data would require a lot more work. But perhaps it can now also be done using AI tools.

```{r}
depression_data <- read_tsv('data/raw_depression.csv')
```

We have to load `stringr` separately.

```{r}
library(stringr)
```

Let's convert the majors to lower case.

```{r}
head(depression_data$major, 10)
depression_data$major <- str_to_lower(depression_data$major)
head(depression_data$major, 10)
```

Let's try to find biology majors. The code below creates a variable that is `TRUE` if the major contains the string `bio`. Next, we filter the data to only include biology students using this new variable.

```{r}
depression_data$bio <- str_detect(depression_data$major, 'bio')
bio_students <- filter(depression_data, bio)
tabyl(bio_students$major)
```

That matched also biomedical, biochemistry students, and others which we might not want to include. Let's detect those using a regex (See <https://regex101.com/> for a regex construction tool).

The code below creates a variable that is `TRUE` if the major contains does *not* contain `chem`, `med`, `phys`, or `tech`.

```{r}
depression_data$not_other <- !str_detect(depression_data$major, 'chem|med|phys|tech')
```

Let's combine our two new columns. We multiply the two columns into a new variable `selection` to get a new column that is `TRUE` if the major contains `bio` and does not contain `chem`, `med`, `phys`, or `tech`. 

```{r}
depression_data <- mutate(depression_data, selection = not_other * bio)
bio_students <- filter(depression_data, selection==1)
tabyl(bio_students$major)
```

## Time using `lubridate`

The ability to work with time is useful for many data sets. But this is not trivial. For example, dates and times are specified in different formats in different countries. And time zones can be a real headache.

The `lubridate` package is a great package for working with dates. It makes it easy to extract parts of a date, calculate differences between dates, and much more. We will cover some basic operations here.

```{r}
library(nycflights13)
library(lubridate)

# Load the flights dataset
data("flights")
# Inspect the structure
glimpse(flights)
# View the first few rows
head(flights)
```

This is the contents of the data:

| **Column**         | **Description**                                                                 |
|--------------------|----------------------------------------------------------------------------------|
| `year`             | Year of the flight (all flights are from 2013).                                   |
| `month`            | Month of the flight (1 = January, 2 = February, etc.).                            |
| `day`              | Day of the month when the flight departed.                                        |
| `dep_time`         | Actual departure time in **local time** (HHMM format, e.g., 517 = 5:17 AM).       |
| `sched_dep_time`   | Scheduled departure time in **local time** (HHMM format).                         |
| `dep_delay`        | Departure delay in **minutes** (positive = delayed, negative = early).            |
| `arr_time`         | Actual arrival time in **local time** (HHMM format).                              |
| `sched_arr_time`   | Scheduled arrival time in **local time** (HHMM format).                           |
| `arr_delay`        | Arrival delay in **minutes** (positive = delayed, negative = early).              |
| `carrier`          | Two-letter airline carrier code (e.g., "AA" for American Airlines).               |
| `flight`           | Flight number.                                                                    |
| `tailnum`          | Tail number (aircraft registration number).                                       |
| `origin`           | Origin airport code (JFK = John F. Kennedy, LGA = LaGuardia, EWR = Newark).       |
| `dest`             | Destination airport code (e.g., "LAX" for Los Angeles).                           |
| `air_time`         | Flight time in **minutes**.                                                       |
| `distance`         | Distance flown in **miles**.                                                      |
| `hour`             | Scheduled departure hour (extracted from `sched_dep_time`).                       |
| `minute`           | Scheduled departure minute (extracted from `sched_dep_time`).                     |
| `time_hour`        | Scheduled departure time as a **POSIXct datetime object** (in local time zone).   |

### Creating a date from parts

The `make_date()` function creates a date from parts. It takes the year, month, and day as arguments. There is also a function `make_datetime()` that includes the time. This function assumes that the time is in 24-hour format and returns the time as UTC.

**No place on Earth uses UTC as its local time zone. However, several places operate on a time zone equivalent to UTC, meaning the local time matches UTC.**

We will use this function together with the `mutate()` function to create a new column `date` in the `flights` dataset.

```{r}
flights <- mutate(flights, my_date1 = make_date(year, month, day))
flights <- mutate(flights, my_date2 = make_date(year, month))
flights <- mutate(flights, my_date3 = make_datetime(year, month, day))
```

If we don't specify a day, the first day of the month is assumed.
When using the `make_time()` function and we don't specify a time, midnight is assumed.

```{r}
flights$my_date1[10000]
flights$my_date2[10000]
flights$my_date3[10000]
```

For fun, let's also add the time of the flight to the date (hours and minutes). To get the hour, we will divide the `dep_time` by 100 and round down. To get the minutes, we will take the remainder of the division by 100.

```{r}
flights <- mutate(flights, hour = floor(dep_time / 100))
flights <- mutate(flights, minute = dep_time %% 100)
flights <- mutate(flights, my_date4 = make_datetime(year, month, day, hour, minute))
flights$my_date4[10000]
```

### Specifying the time zone

The `airport` dataset contains information about the airports. The `tz` column contains the time zone of the airport. We can use this information to specify the time zone of the `my_date4` column.

```{r}
data(airports)
# This is a line we will see later. Ignore it for now.
# It adds data from the airports dataset to the flights dataset.
flights_with_tz <- left_join(flights, airports, by = c("origin" = "faa"))
# The flights dataset now contains the time zone of the origin airport (tzone variable).

flights_with_tz <- mutate(flights_with_tz, my_date4 = make_datetime(year, month, day, hour, minute, tz = tzone))
flights_with_tz$my_date4[10000]
```

### Extracting parts of a date or time

Our `date` column is a `POSIXct` object. This implies that R knows that this column contains dates and times, and it understands the format. The following code shows that the column is indeed a `POSIXct` object.

```{r}
class(flights_with_tz$my_date4)
```


Therefore, We can directly extract parts of this object using the `year()`, `month()`, `day()`, `hour()`, `minute()`, and `second()` functions.

```{r}
flights_with_tz <- mutate(flights_with_tz, extracted_year = year(my_date4))
```

However, if you read in a data file, times and dates are not always recognized as such. In that case, you can use the `ymd()`, `mdy()`, `dmy()`, `ymd_hms()`, `mdy_hms()`, and `dmy_hms()` functions to convert the data to a `POSIXct` object. 

We will demonstrate this using the time_hour column in the `flights` dataset. This column contains the scheduled departure time as a `POSIXct` object. However, we will convert it to a character vector (text) and then back to a `POSIXct` object.

```{r}
flights <- mutate(flights, time_text = as.character(time_hour))
# To make it clear that we are converting the existing posixct object to a character vector, we will also replace : with * and - with +, and spaces with _.
flights <- mutate(flights, time_text = str_replace_all(time_text, ':', '*'))
flights <- mutate(flights, time_text = str_replace_all(time_text, '-', '+'))
flights <- mutate(flights, time_text = str_replace_all(time_text, ' ', '_'))
```

Now we will convert the `time_text` column to a `POSIXct` object using the `ymd_hms()` function. We use this function as the format is `year+month+day_ hour*minute*second`. The `ymd_hms()` function is smart enough to recognize this format, even though we have replaced the `:` with `*` and the `-` with `+`.

```{r}
flights <- mutate(flights, new_date_time = ymd_hms(time_text))
flights$new_date_time[10000]
```

Note that we lost the time zone information when we converted the `time_hour` column to a character vector. Therefore, the `new_date_time` column does not contain time zone information. `ymd_hms()` can recognize and correctly interpret a time zone if it's provided in the string itself, but the string must follow a specific ISO 8601 format or include a valid time zone abbreviation/offset. 

Here, we first add the strings from the 
```{r}
flights_with_tz <- mutate(flights_with_tz, time_text = as.character(time_hour))
flights_with_tz <- mutate(flights_with_tz, time_text = str_replace_all(time_text, ':', '*'))
flights_with_tz <- mutate(flights_with_tz, time_text = str_replace_all(time_text, '-', '+'))
flights_with_tz <- mutate(flights_with_tz, time_text = str_replace_all(time_text, ' ', '_'))

# Now append the time zone to the string
# We will use the tz variable that specifies an offset from UTC in hours and minutes.
# First we need to make sure that -5 is written as -05, etc. This is required for the ISO 8601 format.

converted <- sprintf("%+03d", flights_with_tz$tz)
flights_with_tz <- mutate(flights_with_tz, time_text = paste0(time_text, "*", converted))
flights_with_tz$time_text[1000]
```

Now we can demonstrate that `ymd_hms()` can recognize the time zone if it's provided in the string. Note that the time is converted to the UTC time zone.

```{r}
flights_with_tz$time_text[1000]
flights_with_tz <- mutate(flights_with_tz, new_date_time = ymd_hms(time_text))
flights_with_tz$new_date_time[1000]
```
### Calculating Differences Between Dates/Times

Once R recognizes a time and date as such, calculating differences between dates and times is easy. The `difftime()` function calculates the difference between two dates or times. The result is a `difftime` object that contains the difference in seconds. You can convert this to other units using the `units` argument.

Let's start with a silly example. Say, we want to know how long after January 1, 2012 (noon), the each flight in the `flights` dataset was scheduled.

```{r}
# Let's create the date January 1, 2013
start_date <- ymd_hms("2012-01-01 12:00:00")
flights <- mutate(flights, time_diff = difftime(my_date4, start_date, units = "hours"))
head(flights$time_diff)
```

Let's create a POSIXct time for the scheduled and the actual take off time.

```{r}
# First create the planned_hour and planned_minute columns
flights <- mutate(flights, planned_hour = floor(sched_dep_time / 100))
flights <- mutate(flights, planned_minute = sched_dep_time %% 100)
# Now create the planned_time column
flights <- mutate(flights, planned_time = make_datetime(year, month, day, planned_hour, planned_minute))

# Now create the actual_hour and actual_minute columns
flights <- mutate(flights, actual_hour = floor(dep_time / 100))
flights <- mutate(flights, actual_minute = dep_time %% 100)
# Now create the actual_time column
flights <- mutate(flights, actual_time = make_datetime(year, month, day, actual_hour, actual_minute))
```

Finally we can calculate the difference between the planned and actual take off time. Here, we'll calculate that difference in minutes.

```{r}
flights <- mutate(flights, time_diff = difftime(actual_time, planned_time, units = "mins"))
head(flights$time_diff)
```

Let's look at the histogram of the time differences.

```{r}
# Convert the time_diff column to numeric
time_diff <- as.numeric(flights$time_diff)
# Create a histogram
# Note that there are missing data. This is because there is missing data in the original data.
summary(time_diff)
hist(time_diff, breaks=100)
```
### Filtering Data Based on Date/Time Ranges

The last thing we will see is how to filter data based on date/time ranges. This is done using the `filter()` function.

Let's say we want to filter the `flights` dataset to only include flights that were scheduled between May 15 and May 20, 2013.

```{r}
# Create the lower date
lower_date <- ymd("2013-05-15")
# Create the upper date
upper_date <- ymd("2013-05-20")
# Filter the data
filtered_flights <- filter(flights, my_date4 >= lower_date & my_date4 <= upper_date)
```

Now, we want to filter flights that were scheduled between 8:00 AM on May 15 and 10:00 AM on May 20.

```{r}
# Create the lower datetime
lower_date <- ymd_hms("2013-05-15 08:00:00")
# Create the upper datetime
upper_date <- ymd_hms("2013-05-20 10:00:00")
# Filter the data
filtered_flights <- filter(flights, my_date4 >= lower_date & my_date4 <= upper_date)
```

**Note that this last operation uses the UTC time.**

## Reorganizing data

Now, we will cover how to reorganize data. This is often necessary when you want to summarize data or when you want to plot data. This is often the final step before running the actual analysis.

### Loading some data

```{r}
car_data <- read_delim('data/cars.txt', delim = ' ')
```
### Grouping and summarizing data

The `group_by()` function takes a tibble and returns the same tibble, but with some extra information so that any subsequent function acts on each unique combination defined in the `group_by()`.

```{r}
grouped <- group_by(car_data, type, make)
```

If you inspect the `grouped` tibble it seems the same as the `car_data` tibble. However, it has stored the grouping we asked for.

```{r}
groups(grouped)
```

Now, you can use the `summarize()` function to get summary data for each subgroup

```{r}
summaries <- summarise(grouped, mean.length = mean(length))
summaries
```

You can ask for more than one summary statistic.

```{r}
summaries <- summarise(grouped, mean.length = mean(length), max.length = max(length), std_rpm = sd(rpm))
summaries
```


### Further processing

Keep in mind that, as the result of `summarize()` is a tibble, you can use the data selection methods we saw earlier.

```{r}
subset <- filter(summaries, make ==  'Ford')
head(subset)
```

### Note on dropping non-existing levels

By default, the `group_by()` function drops non-existing combinations of grouping variables.

```{r}
grouped1 <- group_by(car_data, type, make)
c1<-count(grouped1)
summaries1 <- summarise(grouped, mean.length = mean(length))
dim(summaries1)
```

Note: The message ``summarise()` has grouped output by 'type'. You can override using the `.groups` argument.`` just reminds us we grouped the data and the the summary statistics are calculated for each group -- which is exactly what we wanted.

However, this behavior can be changed using the `.drop = FALSE` argument (and converting the variables to factors)

```{r}
grouped2 <- group_by(car_data, as.factor(type), as.factor(make), .drop=FALSE)
c2<-count(grouped2)
summaries2 <- summarise(grouped2, mean.length = mean(length))
summaries2 <- complete(summaries2)
dim(summaries2)
```

### Converting to wide format

The result can be reshaped into a wide format. *While this format is often not suited for plotting or analysis*, it might make it easier to look at the data. Here is a quick visual:

![](images/pivot_wider_new.png) 

+ **names_from: The variable whose values will become the new column names.**

Example: If you have a month column, each unique month value (e.g., "January", "February") will become a new column.

+ **values_from: The variable whose values will populate the cells of the new columns.**

Example: If you have a sales column, the values from this column will fill the corresponding cells in the new columns.

+ **id_cols: The variables used as row identifiers.**

Example: If you have a store column, each row will correspond to a unique store

The function also takes other arguments to handle more complex cases, see <https://tidyr.tidyverse.org/reference/pivot_wider.html>

```{r}
wide <- pivot_wider(summaries, id_cols = make, names_from  = type, values_from = mean.length)
head(wide)
```

### Example: Titanic data

```{r}
titanic <-read_csv('data/Titanic.csv', na='*')
grouped <- group_by(titanic, PClass, Sex)
survival <- summarise(grouped, proportion = mean(Survived), survivors = sum(Survived), total = length(Survived))
survival
```

You could make the result into a wide table.

```{r}
survival_wide <- pivot_wider(survival,  names_from = Sex, id_cols = PClass, values_from = proportion)
survival_wide
```


### Making data longer (melting data)

Here is a quick graphic:

![](images/pivot_longer_new.png)

Let's look at some data:

```{r}
head(relig_income, 5)
```

This data is in a wider format. But we can easily melt it to a long format.

```{r}
new <- pivot_longer(relig_income, cols = !religion, names_to = "income", values_to = "count")
head(new, 5)
```

+ **`cols`**: The columns you want to pivot from **wide to long**.  
   - *Example*: If you have columns like `"January"`, `"February"`, and `"March"`, you would list these here to combine them into one.

+ **`names_to`**: The name of the **new column** that will store the names of the original columns.  
   - *Example*: If your original columns represent months, the new column could be called `"month"`.

+ **`values_to`**: The name of the **new column** that will store the values from the original columns.  
   - *Example*: If the original columns contain sales data, this new column could be called `"sales"`.


## Merging data

Merging is required if your data is spread across different tibbles. This might be the case if you have different data sources or if you have data that is split into different files. However, it might also arise when you've split an existing data set in different tibbles for preprocessing and analysis and then you want to merge the results back together.

Here, we will use National Health and Nutrition Examination Survey data. This is the [data source](https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2017). See [here](https://wwwn.cdc.gov/Nchs/Nnyfs/Y_DEMO.htm) for more information about the demographic variables.

These data are split into multiple files (based on topic). If we need variables from different files, we can merge the different data subsets.

### Reading in the data

The data is in SAS Transport File Format. Therefore, we will use the `haven` library to read it in.

```{r}
library(haven)
demographics <- read_xpt('data/DEMO_J.XPT')
income <- read_xpt('data/INQ_J.XPT')
drugs <- read_xpt('data/DUQ_J.XPT')
```

There are many variables in each file. To keep things manageable, let's select some variables (we've seen how to do this before).

```{r}
drug_habits <- select(drugs, SEQN, DUQ200:DUQ280)
age<-select(demographics, SEQN, RIDAGEYR, RIDAGEMN)
finance <- select(income, SEQN, INDFMMPC,INDFMMPI, INQ300, IND235)
```

### Merging the data

```{r}
merged <- full_join(age, drug_habits, by='SEQN')
merged <- full_join(merged, finance, by='SEQN')
dim(merged)
```

Other related operations are illustrated in the image below. The various operations differ in the way they handle rows missing in the left or right tibble. In the image below, the merge is done by the variable `ID`.

![](images/join.png)

## Example: Global Health Data

Use the following data for this exercise:

```{r}
library(gapminder)
gap_data <- gapminder
```

### Questions A

Starting from the original `gap_data`,

-   Filter the data for the Americas in 2007.
-   Create the variable `gdp`, defined as the product of `pop` and `gdpPercap`.

### Questions B

Starting from the original `gap_data`,

-   Compute the mean life expectancy for each year per continent.  
-   Identify all observations with above average life expectancy, stratified for each continent and year.

### Solutions A

```{r}
head(gap_data)
```

```{r}
colnames(gap_data)
```

```{r}
americas <- filter(gap_data, continent == 'Americas')
americas <- mutate(americas, gdp = (gdpPercap * pop)/1000000)

```

### Solutions B

```{r}
grouped <- group_by(gap_data, continent, year)
summaries <- summarise(grouped, mean.lifeExp = mean(lifeExp))
summaries
```

```{r}
merged <- full_join(gap_data, summaries, by=c('continent', 'year'))
head(merged)
```

```{r}
merged <- mutate(merged, above.mean = lifeExp > mean.lifeExp)
head(merged)
```


## Example: Social Security Applications

I got this example from [here](https://www.linkedin.com/learning-login/share?account=2133849&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Fdata-wrangling-in-r-14135737%3Ftrk%3Dshare_ent_url%26shareId%3D%252BDp0PBL0TaS6XMBZDGEN2w%253D%253D). This data set tracks how many people apply for SS through the website (internet) and the total applications (including applications through mail).

**Take some time to think how you would convert this data to a tidy data set.**

```{r}
ss <- read_csv('data/ssadisability.csv')
glimpse(ss)
colnames(ss)
```

### A reminder on `pivot_longer()`

![](images/pivot_longer_new.png)

### Using `pivot_longer()`

```{r}
longer_format <- pivot_longer(ss, cols=Oct_Total:Sept_Total, names_to = 'period_source', values_to = 'Count')
```

This is better. But not perfect yet.

### Using `separate()`

Let's split the `period_source` column.

```{r}
splitted <- separate(longer_format, period_source, into=c('Month', 'Source'), sep='_')
```

### Creating an `Other` column

I'd like to create a column listing the difference between `Internet` and `Total`. I'll do this by first creating a wider format.

![](images/pivot_wider_new.png)

```{r}
wider <- pivot_wider(splitted, names_from = 'Source', values_from = 'Count')
head(wider)
wider<-mutate(wider, Other = Total - Internet)
head(wider)
```

Now we can go back to the long format.

```{r}
long_again <- select(wider, -Total)
long_again <- pivot_longer(long_again, cols = c(Internet, Other), names_to = 'Source', values_to = 'Count') 
```

### Convert `Fiscal_Year` to year

```{r}
numbers <- str_extract(long_again$Fiscal_Year, '[0-9]{2}')
numbers <- 2000 + as.integer(numbers)
long_again <- add_column(long_again, Year = numbers)
long_again <- select(long_again, -Fiscal_Year)
```

### Create a date variable

```{r}
library(lubridate)
long_again$date <- paste('01', long_again$Month, long_again$Year)
long_again$date<-dmy(long_again$date)
```

### Plots

Let's create some plots (We'll dedicate some more time to plotting in a next section of the course).

```{r}
data_2012 <- filter(long_again, Year == 2012)
ggplot(data_2012) + aes(x=date, y = Count, group=Source, color=Source) + geom_line()
```

```{r}
ggplot(long_again) + aes(x=Month, y = Count, group=Source, color=Source) + geom_line() + facet_grid(~Year)
```

## Example: Coal data

Have a look at the data in an editor: + The first 2 lines can be skipped. + Missing values are given by `--`.

```{r}
coal_data <- read_csv('data/coal.csv', skip=2, na = "--")
head(coal_data)
```

**Take some time to think how you would convert this data to a tidy data set.**

### Change the first variable name

```{r}
existing_names <- colnames(coal_data)
existing_names[1] <- 'country'
colnames(coal_data) <- existing_names
```

### Convert to a long data format

![](images/pivot_longer_new.png)

```{r}
long_format <- pivot_longer(coal_data, names_to = 'year', values_to = 'coal_use', cols = '1980':'2009')
```

### Separate regions and countries

```{r}
regions <- c("North America", "Central & South America", "Antarctica", "Europe", "Eurasia", "Middle East", "Africa", "Asia & Oceania", "World")

long_format <- mutate(long_format, is_region =long_format$country %in% regions)

region_data <- filter(long_format, is_region)
country_data <- filter(long_format, !is_region)

# Remove the 'is_region' column from both tibbles
region_data <- select(region_data, -is_region)
country_data <- select(country_data, -is_region)
```

## Further exercises

I have provided two data sets for some quick exercises:

### Voice-onset data

These data (`vot.csv`) were taken from [here](https://www.jvcasillas.com/untidydata/). This is a voice-onset time data set. Includes coronal stop data from English and Spanish monolinguals, as well as English/Spanish bilinguals. In these data, the `participant` variable denotes the participant's number and whether they are monolingual (Spanish or English) or bilingual.

**Try to reformat the data such that it features (1) a column with the participant's number (0-9) and (2) a column indicating whether the participant is a Spanish speaker, English Speaker or Bilingual.**

### Airline safety data

I took these data (`airline-safety.csv`) from [here](https://github.com/fivethirtyeight/data/blob/master/airline-safety/airline-safety.csv). This is data on airline safety from 1985-1999 and 2000-2014. As you can see in the data, the column names can be considered as variables indicating the period and the specific statistic for the period (number of incidents and fatalities).

**Try rearranging the data to contain a column `period` listing either 1985-1999 or 2000-2014. The data should contain four further columns: `avail_seat_km_per_week`, `incidents`, `fatal_accidents` and `fatalities`.**
